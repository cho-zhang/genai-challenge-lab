# Experiment configuration for NIST Code Pilot challenge
# This file controls which LLM model, parameters, and prompts to use

# File paths (relative to current working directory)
#input_json_path: "2026_code/genai_code_pilot_data_participant/input_dry_v2d00.json"
input_json_path: "2026_code/genai_code_pilot_data_participant/input_pilot_v2d00.json"
output_folder: "2026_code/submissions"

# LLM configuration
llm_config:
  # ============ GPT 5.2 + default reasoning ============ #
   model_name: "gpt-5.2"
   max_completion_tokens: 4000
   reasoning_effort: "medium"

  # ============ GPT 5 mini + minimal reasoning ============ #
  # model_name: "gpt-5-mini"
  # temperature: 0.0
  # litellm.exceptions.ContextWindowExceededError: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - This model's maximum context length is 8192 tokens. However, you requested 8220 tokens (220 in the messages, 8000 in the completion).
  # 10031_compute_list_values
  # max_completion_tokens: 3000
  # Literal["none", "minimal", "low", "medium", "high", "xhigh", "default"]
  # reasoning_effort: "minimal"
  # top_p: 1.0
  # tool_choice: "auto"

  # ============ GPT 4 + no reasoning ============ #
  # model_name: "gpt-4"
  # temperature: 0.0
  # max_completion_tokens: 500

  # ============ Haiku ============ #
  #model_name: "claude-haiku-4-5"
  #max_completion_tokens: 3000

  # ============ Sonnet ============ #
  # model_name: "claude-sonnet-4-5"
  # max_completion_tokens: 3000

# Prompt configuration
# Use the fixed prompt from NIST input data
# prompt_name: "prompt_fixed"
# prompt_version: "2.00"
# Or use custom prompt from registry:
prompt_name: "custom_prompt"
prompt_version: "6"
